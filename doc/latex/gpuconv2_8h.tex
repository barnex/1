\hypertarget{gpuconv2_8h}{
\section{gpuconv2.h File Reference}
\label{gpuconv2_8h}\index{gpuconv2.h@{gpuconv2.h}}
}
{\ttfamily \#include \char`\"{}tensor.h\char`\"{}}\par
{\ttfamily \#include \char`\"{}gputil.h\char`\"{}}\par
{\ttfamily \#include $<$cufft.h$>$}\par
{\ttfamily \#include \char`\"{}gpufft2.h\char`\"{}}\par
\subsection*{Data Structures}
\begin{DoxyCompactItemize}
\item 
struct \hyperlink{structgpuconv2}{gpuconv2}
\end{DoxyCompactItemize}
\subsection*{Functions}
\begin{DoxyCompactItemize}
\item 
\hyperlink{structgpuconv2}{gpuconv2} $\ast$ \hyperlink{gpuconv2_8h_a276cde10877794d8927911fe2abeec0f}{new\_\-gpuconv2} (int $\ast$size, int $\ast$kernelSize)
\item 
void \hyperlink{gpuconv2_8h_a517ed453bc81ab528138f9d7624efffe}{gpuconv2\_\-loadkernel5DSymm} (\hyperlink{structgpuconv2}{gpuconv2} $\ast$conv, \hyperlink{structtensor}{tensor} $\ast$kernel5D)
\item 
void \hyperlink{gpuconv2_8h_a9846e091be7ddd707f1325cd50d0c455}{gpuconv2\_\-loadkernel2DSymm} (\hyperlink{structgpuconv2}{gpuconv2} $\ast$conv, \hyperlink{structtensor}{tensor} $\ast$kernel2D)
\item 
void \hyperlink{gpuconv2_8h_ada13dcbed7b84f58bafbf2a0bc57ae85}{gpuconv2\_\-exec} (\hyperlink{structgpuconv2}{gpuconv2} $\ast$plan, \hyperlink{structtensor}{tensor} $\ast$source, \hyperlink{structtensor}{tensor} $\ast$dest)
\item 
void \hyperlink{gpuconv2_8h_ac011c1c90f174de0957253c215a14e57}{gpuconv2\_\-loadkernel} (\hyperlink{structgpuconv2}{gpuconv2} $\ast$plan, \hyperlink{structtensor}{tensor} $\ast$kernel)
\item 
void \hyperlink{gpuconv2_8h_a6fbd8e2f7bceed3b1c88b22c93fbc7f9}{gpu\_\-kernel\_\-mul2} (float $\ast$ft\_\-m\_\-i, float $\ast$ft\_\-kernel\_\-comp\_\-ij, float $\ast$ft\_\-h\_\-comp\_\-j, int nRealNumbers)
\item 
void \hyperlink{gpuconv2_8h_ac7cea8323497b8202392c49c909b6803}{gpu\_\-kernel\_\-mul\_\-complex\_\-inplace\_\-symm} (float $\ast$fftMx, float $\ast$fftMy, float $\ast$fftMz, float $\ast$fftKxx, float $\ast$fftKyy, float $\ast$fftKzz, float $\ast$fftKyz, float $\ast$fftKxz, float $\ast$fftKxy, int nRealNumbers)
\item 
void \hyperlink{gpuconv2_8h_a2df60637926ff063cab034ec68d0dae5}{gpuconv2\_\-copy\_\-pad} (\hyperlink{structgpuconv2}{gpuconv2} $\ast$conv, float $\ast$source, float $\ast$dest)
\item 
void \hyperlink{gpuconv2_8h_a16e5077d5fca74094114eff78e98ea5a}{gpuconv2\_\-copy\_\-unpad} (\hyperlink{structgpuconv2}{gpuconv2} $\ast$conv, float $\ast$source, float $\ast$dest)
\end{DoxyCompactItemize}


\subsection{Detailed Description}
A smarter vector convolution plan on the GPU: real-\/to-\/complex FFT's. The redundant zero's in the padded magnetization buffers are ignored. The zero's in the micromagnetic kernel are ignored. Care is taken to align CUDA memory access.

The interface is flexible: gpuconv2\_\-exec(m, h) can be called on any magnetization and field array that match the size of the plan. m and h are thus not stored in the plan itself. This is handy for higher order solvers that keep multiple versions of m and h.

\begin{Desc}
\item[\hyperlink{todo__todo000006}{Todo}]TODO voor convolutie (Ben)\end{Desc}
1. Greense functies -\/$>$ dienen gegenereerd te worden in strided formaat -\/$>$ is symmetrische rank 4 tensor (vb: gxy = gyx, gxz = gzx, ..., slechts 2/3 van geheugen nodig) -\/$>$ Enkel reeel deel in Fourier domein (halveert geheugen vereisten) -\/$>$ implementatie algemene Greense tensor nodig met als input te gebruiken Greense functie -\/$>$ Er dient rekening gehouden te worden met mogelijke periodiciteit

2. seriele berekening veldwaarden gunstiger -\/$>$ beter seriele berekening van H\_\-x, H\_\-y, H\_\-z als a. H$^\wedge$FFT = g$^\wedge$FFT\_\-xx$\ast$ m$^\wedge$FFT\_\-x + g$^\wedge$FFT\_\-xy$\ast$ m$^\wedge$FFT\_\-y + g$^\wedge$FFT\_\-xz$\ast$ m$^\wedge$FFT\_\-z b. H\_\-x = inv\_\-FFT(H$^\wedge$FFT) c. H$^\wedge$FFT = g$^\wedge$FFT\_\-xy$\ast$ m$^\wedge$FFT\_\-x + g$^\wedge$FFT\_\-yy$\ast$ m$^\wedge$FFT\_\-y + g$^\wedge$FFT\_\-yz$\ast$ m$^\wedge$FFT\_\-z d. H\_\-y = inv\_\-FFT(H$^\wedge$FFT) e. H$^\wedge$FFT = g$^\wedge$FFT\_\-xz$\ast$ m$^\wedge$FFT\_\-x + g$^\wedge$FFT\_\-yz$\ast$ m$^\wedge$FFT\_\-y + g$^\wedge$FFT\_\-zz$\ast$ m$^\wedge$FFT\_\-z f. H\_\-z = inv\_\-FFT(H$^\wedge$FFT) Op die manier enkel geheugen nodig voor H$^\wedge$FFT (en niet voor elke component H$^\wedge$FFT\_\-x, H$^\wedge$FFT\_\-y, H$^\wedge$FFT\_\-z) Antw: Ik denk dat ik nu slechts even veel geheugen gebruik: Ik houd 3 H$^\wedge$FFT componenten in het geheugen, maar slechts één m$^\wedge$FFT component, jij één H$^\wedge$FFT maar 3 m$^\wedge$FFT's. Of heb ik het mis op? (Arne.) Opm: misschien kunnen we wel één buffer uitsparen door eerst alle m\_\-i te FFT-\/en en dan een \char`\"{}in-\/place\char`\"{} kernel vermenigvuldiging te doen. Per element wordt dan m\_\-x\mbox{[}i\mbox{]}, m\_\-y\mbox{[}i\mbox{]}, m\_\-z\mbox{[}i\mbox{]} gebufferd in locale variablen, daarna wordt m$^\wedge$FFT element per element overschreven door H$^\wedge$FFT...

-\/$>$ H$^\wedge$FFT dient dezelfde dimensies te hebben als andere strided FFT tensoren

3. Transponeren matrices -\/$>$ is versnelling mogelijk door nullen niet te transponeren? -\/$>$ In place transponeren

4. Omtrent de FFT routines -\/$>$ Waarschijnlijk beter om FFT routines in een aparte bibliotheek te steken wegens mogelijk gebruik in andere convoluties -\/$>$ implementatie 2D varianten: Uitbreiding van de huidige routines of aparte routines? mogelijkheden: a. Aparte routines voor 3D en 2D: bij aanroepen if constructies nodig (if 3D, if 2D) b. uitbreiding routines:
\begin{DoxyItemize}
\item extra argument 2D of 3D, met daarna daarna twee totaal verschillende code blokken
\item geen extra argumenten, maar op basis van dimensies in argument.
\end{DoxyItemize}

\begin{DoxySeeAlso}{See also}
\hyperlink{structgpuconv1}{gpuconv1}, \hyperlink{gpuconv2_8h_a276cde10877794d8927911fe2abeec0f}{new\_\-gpuconv2}, \hyperlink{gpuconv2_8h_ada13dcbed7b84f58bafbf2a0bc57ae85}{gpuconv2\_\-exec}
\end{DoxySeeAlso}
\begin{Desc}
\item[\hyperlink{todo__todo000007}{Todo}]when NO=1, gxy and gxz are zero\end{Desc}
\begin{DoxyAuthor}{Author}
Arne Vansteenkiste 

Ben Van de Wiele 
\end{DoxyAuthor}


\subsection{Function Documentation}
\hypertarget{gpuconv2_8h_a6fbd8e2f7bceed3b1c88b22c93fbc7f9}{
\index{gpuconv2.h@{gpuconv2.h}!gpu\_\-kernel\_\-mul2@{gpu\_\-kernel\_\-mul2}}
\index{gpu\_\-kernel\_\-mul2@{gpu\_\-kernel\_\-mul2}!gpuconv2.h@{gpuconv2.h}}
\subsubsection[{gpu\_\-kernel\_\-mul2}]{\setlength{\rightskip}{0pt plus 5cm}void gpu\_\-kernel\_\-mul2 (float $\ast$ {\em ft\_\-m\_\-i}, \/  float $\ast$ {\em ft\_\-kernel\_\-comp\_\-ij}, \/  float $\ast$ {\em ft\_\-h\_\-comp\_\-j}, \/  int {\em nRealNumbers})}}
\label{gpuconv2_8h_a6fbd8e2f7bceed3b1c88b22c93fbc7f9}
Pointwise multiplication of arrays of complex numbers. ft\_\-h\_\-comp\_\-j += ft\_\-m\_\-i $\ast$ ft\_\-kernel\_\-comp\_\-ij. Runs on the GPU. Makes use of kernel symmetry \begin{DoxyNote}{Note}
DO NOT store in texture memory! This would be a bit faster on older devices, but actually slower on Fermi cards! 
\end{DoxyNote}

\begin{DoxyParams}{Parameters}
\item[{\em ft\_\-m\_\-i}]multiplication input 1 \item[{\em ft\_\-kernel\_\-comp\_\-ij}]multiplication input 2 \item[{\em ft\_\-h\_\-comp\_\-j}]multiplication result gets added to this array \item[{\em nRealNumbers}]the number of floats(!) in each of the arrays, thus twice the number of complex's in them. \end{DoxyParams}
\hypertarget{gpuconv2_8h_ac7cea8323497b8202392c49c909b6803}{
\index{gpuconv2.h@{gpuconv2.h}!gpu\_\-kernel\_\-mul\_\-complex\_\-inplace\_\-symm@{gpu\_\-kernel\_\-mul\_\-complex\_\-inplace\_\-symm}}
\index{gpu\_\-kernel\_\-mul\_\-complex\_\-inplace\_\-symm@{gpu\_\-kernel\_\-mul\_\-complex\_\-inplace\_\-symm}!gpuconv2.h@{gpuconv2.h}}
\subsubsection[{gpu\_\-kernel\_\-mul\_\-complex\_\-inplace\_\-symm}]{\setlength{\rightskip}{0pt plus 5cm}void gpu\_\-kernel\_\-mul\_\-complex\_\-inplace\_\-symm (float $\ast$ {\em fftMx}, \/  float $\ast$ {\em fftMy}, \/  float $\ast$ {\em fftMz}, \/  float $\ast$ {\em fftKxx}, \/  float $\ast$ {\em fftKyy}, \/  float $\ast$ {\em fftKzz}, \/  float $\ast$ {\em fftKyz}, \/  float $\ast$ {\em fftKxz}, \/  float $\ast$ {\em fftKxy}, \/  int {\em nRealNumbers})}}
\label{gpuconv2_8h_ac7cea8323497b8202392c49c909b6803}
\begin{DoxyInternal}{For internal use only.}
\end{DoxyInternal}
\hypertarget{gpuconv2_8h_a2df60637926ff063cab034ec68d0dae5}{
\index{gpuconv2.h@{gpuconv2.h}!gpuconv2\_\-copy\_\-pad@{gpuconv2\_\-copy\_\-pad}}
\index{gpuconv2\_\-copy\_\-pad@{gpuconv2\_\-copy\_\-pad}!gpuconv2.h@{gpuconv2.h}}
\subsubsection[{gpuconv2\_\-copy\_\-pad}]{\setlength{\rightskip}{0pt plus 5cm}void gpuconv2\_\-copy\_\-pad ({\bf gpuconv2} $\ast$ {\em conv}, \/  float $\ast$ {\em source}, \/  float $\ast$ {\em dest})}}
\label{gpuconv2_8h_a2df60637926ff063cab034ec68d0dae5}
\begin{DoxyInternal}{For internal use only.}
Copies 3D data to a zero-\/padded, strided destination. Runs on the GPU. \end{DoxyInternal}

\begin{DoxyParams}{Parameters}
\item[{\em conv}]this convolution plan contains the sizes of both arrays \item[{\em source}]source data on GPU, should have size: conv-\/$>$size \item[{\em dest}]destination data on GPU, should have size: conv-\/$>$paddedStorageSize \end{DoxyParams}
\hypertarget{gpuconv2_8h_a16e5077d5fca74094114eff78e98ea5a}{
\index{gpuconv2.h@{gpuconv2.h}!gpuconv2\_\-copy\_\-unpad@{gpuconv2\_\-copy\_\-unpad}}
\index{gpuconv2\_\-copy\_\-unpad@{gpuconv2\_\-copy\_\-unpad}!gpuconv2.h@{gpuconv2.h}}
\subsubsection[{gpuconv2\_\-copy\_\-unpad}]{\setlength{\rightskip}{0pt plus 5cm}void gpuconv2\_\-copy\_\-unpad ({\bf gpuconv2} $\ast$ {\em conv}, \/  float $\ast$ {\em source}, \/  float $\ast$ {\em dest})}}
\label{gpuconv2_8h_a16e5077d5fca74094114eff78e98ea5a}
\begin{DoxyInternal}{For internal use only.}
Copies 3D data from a zero-\/padded and strided destination. Runs on the GPU \end{DoxyInternal}

\begin{DoxyParams}{Parameters}
\item[{\em conv}]this convolution plan contains the sizes of both arrays \item[{\em source}]destination data on GPU, should have size: conv-\/$>$paddedStorageSize \item[{\em dest}]source data on GPU, should have size: conv-\/$>$size \end{DoxyParams}
\hypertarget{gpuconv2_8h_ada13dcbed7b84f58bafbf2a0bc57ae85}{
\index{gpuconv2.h@{gpuconv2.h}!gpuconv2\_\-exec@{gpuconv2\_\-exec}}
\index{gpuconv2\_\-exec@{gpuconv2\_\-exec}!gpuconv2.h@{gpuconv2.h}}
\subsubsection[{gpuconv2\_\-exec}]{\setlength{\rightskip}{0pt plus 5cm}void gpuconv2\_\-exec ({\bf gpuconv2} $\ast$ {\em plan}, \/  {\bf tensor} $\ast$ {\em source}, \/  {\bf tensor} $\ast$ {\em dest})}}
\label{gpuconv2_8h_ada13dcbed7b84f58bafbf2a0bc57ae85}
Executes the convolution plan: convolves the source data with the stored kernel and stores the result in the destination pointer. 
\begin{DoxyParams}{Parameters}
\item[{\em plan}]the plan to execute \item[{\em source}]the input vector field (magnetization) \item[{\em dest}]the destination vector field (magnetic field) to store the result in \end{DoxyParams}
\hypertarget{gpuconv2_8h_ac011c1c90f174de0957253c215a14e57}{
\index{gpuconv2.h@{gpuconv2.h}!gpuconv2\_\-loadkernel@{gpuconv2\_\-loadkernel}}
\index{gpuconv2\_\-loadkernel@{gpuconv2\_\-loadkernel}!gpuconv2.h@{gpuconv2.h}}
\subsubsection[{gpuconv2\_\-loadkernel}]{\setlength{\rightskip}{0pt plus 5cm}void gpuconv2\_\-loadkernel ({\bf gpuconv2} $\ast$ {\em plan}, \/  {\bf tensor} $\ast$ {\em kernel})}}
\label{gpuconv2_8h_ac011c1c90f174de0957253c215a14e57}
Loads a kernel. Automatically called during \hyperlink{gpuconv2_8h_a276cde10877794d8927911fe2abeec0f}{new\_\-gpuconv2()}, but could be used to change the kernel afterwards. \begin{DoxySeeAlso}{See also}
\hyperlink{gpuconv2_8h_a276cde10877794d8927911fe2abeec0f}{new\_\-gpuconv2} 
\end{DoxySeeAlso}

\begin{DoxyParams}{Parameters}
\item[{\em plan}]plan to load the kernel into \item[{\em kernel}]kernel to load (should match the plan size) \end{DoxyParams}
\hypertarget{gpuconv2_8h_a9846e091be7ddd707f1325cd50d0c455}{
\index{gpuconv2.h@{gpuconv2.h}!gpuconv2\_\-loadkernel2DSymm@{gpuconv2\_\-loadkernel2DSymm}}
\index{gpuconv2\_\-loadkernel2DSymm@{gpuconv2\_\-loadkernel2DSymm}!gpuconv2.h@{gpuconv2.h}}
\subsubsection[{gpuconv2\_\-loadkernel2DSymm}]{\setlength{\rightskip}{0pt plus 5cm}void gpuconv2\_\-loadkernel2DSymm ({\bf gpuconv2} $\ast$ {\em conv}, \/  {\bf tensor} $\ast$ {\em kernel2D})}}
\label{gpuconv2_8h_a9846e091be7ddd707f1325cd50d0c455}
Loads a kernel into the convolution. The kernel is FFTed and stored in a 2-\/dimensional format: Kernel\mbox{[}SourceDir\mbox{]}\mbox{[}index\mbox{]}. The kernel has the format discribed in \hyperlink{gpukernel1_8h}{gpukernel1.h} \begin{DoxySeeAlso}{See also}
\hyperlink{gpukernel1_8h}{gpukernel1.h} 
\end{DoxySeeAlso}
\begin{DoxyNote}{Note}
for use with Ben's kernels 
\end{DoxyNote}
\begin{Desc}
\item[\hyperlink{todo__todo000008}{Todo}]not yet implemented \end{Desc}

\begin{DoxyParams}{Parameters}
\item[{\em kernel2D}]Kernel on Device, normalized \end{DoxyParams}
\hypertarget{gpuconv2_8h_a517ed453bc81ab528138f9d7624efffe}{
\index{gpuconv2.h@{gpuconv2.h}!gpuconv2\_\-loadkernel5DSymm@{gpuconv2\_\-loadkernel5DSymm}}
\index{gpuconv2\_\-loadkernel5DSymm@{gpuconv2\_\-loadkernel5DSymm}!gpuconv2.h@{gpuconv2.h}}
\subsubsection[{gpuconv2\_\-loadkernel5DSymm}]{\setlength{\rightskip}{0pt plus 5cm}void gpuconv2\_\-loadkernel5DSymm ({\bf gpuconv2} $\ast$ {\em conv}, \/  {\bf tensor} $\ast$ {\em kernel5D})}}
\label{gpuconv2_8h_a517ed453bc81ab528138f9d7624efffe}
Loads a kernel into the convolution. The kernel is not yet FFTed and stored in the 5-\/dimensional format: Kernel\mbox{[}SourceDir\mbox{]}\mbox{[}DestDir\mbox{]}\mbox{[}X\mbox{]}\mbox{[}Y\mbox{]}\mbox{[}Z\mbox{]}. The kernel is assumed to be symmetric in the first two indices. \begin{DoxyNote}{Note}
for use with Arne's kernels. 
\end{DoxyNote}

\begin{DoxyParams}{Parameters}
\item[{\em kernel5D}]Kernel on Host, not yet normalized \end{DoxyParams}
\hypertarget{gpuconv2_8h_a276cde10877794d8927911fe2abeec0f}{
\index{gpuconv2.h@{gpuconv2.h}!new\_\-gpuconv2@{new\_\-gpuconv2}}
\index{new\_\-gpuconv2@{new\_\-gpuconv2}!gpuconv2.h@{gpuconv2.h}}
\subsubsection[{new\_\-gpuconv2}]{\setlength{\rightskip}{0pt plus 5cm}{\bf gpuconv2}$\ast$ new\_\-gpuconv2 (int $\ast$ {\em size}, \/  int $\ast$ {\em kernelSize})}}
\label{gpuconv2_8h_a276cde10877794d8927911fe2abeec0f}
New convolution plan with given size of the source vector field and kernel. If the kernel size is larger than the vector field, the field is zero-\/padded in the respective dimension to fit the size of the kernel. \begin{DoxyNote}{Note}
After construction, a kernel should still be loaded. 
\end{DoxyNote}

\begin{DoxyParams}{Parameters}
\item[{\em size}]X Y and Z size of the magnetization vector field \item[{\em kernelSize}]convolution kernel size of at least the size of the vector field \end{DoxyParams}
