\hypertarget{gputil_8h}{
\section{gputil.h File Reference}
\label{gputil_8h}\index{gputil.h@{gputil.h}}
}
{\ttfamily \#include \char`\"{}tensor.h\char`\"{}}\par
{\ttfamily \#include $<$cufft.h$>$}\par
{\ttfamily \#include $<$stdlib.h$>$}\par
{\ttfamily \#include $<$stdarg.h$>$}\par
\subsection*{Functions}
\begin{DoxyCompactItemize}
\item 
float $\ast$ \hyperlink{gputil_8h_ac0a2a829eda78d8c61d65abe004c125e}{new\_\-gpu\_\-array} (int size)
\item 
\hyperlink{structtensor}{tensor} $\ast$ \hyperlink{gputil_8h_a7c2b7f29fd636e841b2001b07ffb5d18}{new\_\-gputensor} (int rank, int $\ast$size)
\item 
float $\ast$ \hyperlink{gputil_8h_a9bad0e9d5c5e49d54cc7f681954a990c}{new\_\-ram\_\-array} (int size)
\item 
int \hyperlink{gputil_8h_ade18ea7bbea940fcdc15c47f6d05b3e7}{gpu\_\-stride\_\-float} ()
\item 
int \hyperlink{gputil_8h_a39fd2af01f3bfd02cf6001c270f652f8}{gpu\_\-pad\_\-to\_\-stride} (int nFloats)
\item 
void \hyperlink{gputil_8h_a9b1f5575ea6503ba9152c1db1fb28cb3}{gpu\_\-override\_\-stride} (int nFloats)
\item 
void \hyperlink{gputil_8h_a9fc3a83fc4e2798b4a70fe5538d3db8f}{memcpy\_\-to\_\-gpu} (float $\ast$source, float $\ast$dest, int nElements)
\item 
void \hyperlink{gputil_8h_a587c68bc938e76efa6982de793acf53d}{memcpy\_\-from\_\-gpu} (float $\ast$source, float $\ast$dest, int nElements)
\item 
void \hyperlink{gputil_8h_abb3d67b2b6089e1620679ce812e982fa}{memcpy\_\-gpu\_\-to\_\-gpu} (float $\ast$source, float $\ast$dest, int nElements)
\item 
float \hyperlink{gputil_8h_acc9df64ede1f195cd8188a42a937972f}{gpu\_\-array\_\-get} (float $\ast$dataptr, int index)
\item 
void \hyperlink{gputil_8h_a6f2f5fc9528a4f1e2fc22f1ff8ff7a6e}{gpu\_\-array\_\-set} (float $\ast$dataptr, int index, float value)
\item 
void \hyperlink{gputil_8h_a8ca9dcaab221f4cf101e18c9dd64612b}{tensor\_\-copy\_\-to\_\-gpu} (\hyperlink{structtensor}{tensor} $\ast$source, \hyperlink{structtensor}{tensor} $\ast$dest)
\item 
void \hyperlink{gputil_8h_ad212066ef724f8314ca9f50148c7ebba}{tensor\_\-copy\_\-from\_\-gpu} (\hyperlink{structtensor}{tensor} $\ast$source, \hyperlink{structtensor}{tensor} $\ast$dest)
\item 
void \hyperlink{gputil_8h_ab771627ffcf287025e3b9d0a0558c541}{tensor\_\-copy\_\-gpu\_\-to\_\-gpu} (\hyperlink{structtensor}{tensor} $\ast$source, \hyperlink{structtensor}{tensor} $\ast$dest)
\item 
void \hyperlink{gputil_8h_ad3d70add006fc3a817675184ebd29fad}{gpu\_\-zero} (float $\ast$data, int nElements)
\item 
void \hyperlink{gputil_8h_a26e74f76194a27b6f685a840a63fb678}{gpu\_\-zero\_\-tensor} (\hyperlink{structtensor}{tensor} $\ast$t)
\item 
void \hyperlink{gputil_8h_aadb9a19a5d27210b0ed4dc9a04665826}{gpu\_\-safe} (int status)
\item 
void \hyperlink{gputil_8h_a0c42e9df93568af4129f42ef2efbde36}{format\_\-gputensor} (\hyperlink{structtensor}{tensor} $\ast$t, FILE $\ast$out)
\item 
void \hyperlink{gputil_8h_aeb3988935e9d689318508ab7dd127e8e}{assertHost} (float $\ast$pointer)
\item 
void \hyperlink{gputil_8h_aee41d9d859d5ae5e38c04665701c111a}{assertDevice} (float $\ast$pointer)
\item 
void \hyperlink{gputil_8h_aa843a5123b95fbe239b342f375e8a6c2}{gpu\_\-checkconf} (dim3 gridsize, dim3 blocksize)
\item 
void \hyperlink{gputil_8h_a7b70f435878eea3e2c5476581369e37f}{gpu\_\-checkconf\_\-int} (int gridsize, int blocksize)
\item 
void \hyperlink{gputil_8h_adc4d57eeb6772ef2ac09f18e384c439c}{check3dconf} (dim3 gridsize, dim3 blocksize)
\item 
void \hyperlink{gputil_8h_adfe62deb142d77cd6b63e3b6a0bf65ba}{check1dconf} (int gridsize, int blocksize)
\item 
void \hyperlink{gputil_8h_a5f10ebe67635ce652fa229fa9d8ebc78}{make3dconf} (int \hyperlink{gpufft2__test_8cpp_a1a345a1e634b1d40f23b3333f3696366}{N0}, int \hyperlink{gpufft2__test_8cpp_a5a6a26e0827f9640d2eb7201017f19d7}{N1}, int \hyperlink{gpufft2__test_8cpp_adeea8075a0038c985c63d45161804d60}{N2}, dim3 $\ast$gridSize, dim3 $\ast$blockSize)
\item 
void \hyperlink{gputil_8h_a126d013db24c5bdf7552dac353f0e1a8}{make1dconf} (int N, int $\ast$gridSize, int $\ast$blockSize)
\item 
void $\ast$ \hyperlink{gputil_8h_a137892fd843da0d82f459f90bb2af209}{gpu\_\-getproperties} (void)
\item 
void \hyperlink{gputil_8h_a07dc809bc071234d5b1b2af3dd292860}{print\_\-device\_\-properties} (FILE $\ast$out)
\item 
void \hyperlink{gputil_8h_ad00a2a5bb51df697f7abd399431962fd}{print\_\-device\_\-properties\_\-stdout} ()
\end{DoxyCompactItemize}


\subsection{Detailed Description}
This file provides some common functions for the GPU, like allocating arrays on it...

\begin{Desc}
\item[\hyperlink{todo__todo000016}{Todo}]use CudaGetDeviceProperties to obtain the maximum number of threads per block, etc... 

Strided Arrays 

Smart zero-\/padded FFT's: try strided and transposed 

choose between in-\/place and out-\/of-\/place FFT's for best performance or best memory efficiency\end{Desc}
\begin{DoxyAuthor}{Author}
Arne Vansteenkiste 
\end{DoxyAuthor}


\subsection{Function Documentation}
\hypertarget{gputil_8h_aee41d9d859d5ae5e38c04665701c111a}{
\index{gputil.h@{gputil.h}!assertDevice@{assertDevice}}
\index{assertDevice@{assertDevice}!gputil.h@{gputil.h}}
\subsubsection[{assertDevice}]{\setlength{\rightskip}{0pt plus 5cm}void assertDevice (float $\ast$ {\em pointer})}}
\label{gputil_8h_aee41d9d859d5ae5e38c04665701c111a}
\begin{DoxyInternal}{For internal use only.}
Checks if the data resides on the GPU device by copying one float from device to device. A segmentation fault is thrown when the data resides on the host. \end{DoxyInternal}
\hypertarget{gputil_8h_aeb3988935e9d689318508ab7dd127e8e}{
\index{gputil.h@{gputil.h}!assertHost@{assertHost}}
\index{assertHost@{assertHost}!gputil.h@{gputil.h}}
\subsubsection[{assertHost}]{\setlength{\rightskip}{0pt plus 5cm}void assertHost (float $\ast$ {\em pointer})}}
\label{gputil_8h_aeb3988935e9d689318508ab7dd127e8e}
\begin{DoxyInternal}{For internal use only.}
Checks if the data resides on the host by copying one float from host to host. A segmentation fault is thrown when the data resides on the GPU device. \end{DoxyInternal}
\hypertarget{gputil_8h_adfe62deb142d77cd6b63e3b6a0bf65ba}{
\index{gputil.h@{gputil.h}!check1dconf@{check1dconf}}
\index{check1dconf@{check1dconf}!gputil.h@{gputil.h}}
\subsubsection[{check1dconf}]{\setlength{\rightskip}{0pt plus 5cm}void check1dconf (int {\em gridsize}, \/  int {\em blocksize})}}
\label{gputil_8h_adfe62deb142d77cd6b63e3b6a0bf65ba}
Checks if the CUDA 1D kernel launch configuration is valid. CUDA tends to ignore invalid configurations silently, which is painfull for debugging. Uses device properties 
\begin{DoxyParams}{Parameters}
\item[{\em gridsize}]1D size of the thread grid \item[{\em blocksize}]1D size of the trhead blocks on the grid \end{DoxyParams}
\hypertarget{gputil_8h_adc4d57eeb6772ef2ac09f18e384c439c}{
\index{gputil.h@{gputil.h}!check3dconf@{check3dconf}}
\index{check3dconf@{check3dconf}!gputil.h@{gputil.h}}
\subsubsection[{check3dconf}]{\setlength{\rightskip}{0pt plus 5cm}void check3dconf (dim3 {\em gridsize}, \/  dim3 {\em blocksize})}}
\label{gputil_8h_adc4d57eeb6772ef2ac09f18e384c439c}
Checks if the CUDA 3D kernel launch configuration is valid. CUDA tends to ignore invalid configurations silently, which is painfull for debugging. Uses device properties 
\begin{DoxyParams}{Parameters}
\item[{\em gridsize}]3D size of the thread grid \item[{\em blocksize}]3D size of the trhead blocks on the grid \end{DoxyParams}
\hypertarget{gputil_8h_a0c42e9df93568af4129f42ef2efbde36}{
\index{gputil.h@{gputil.h}!format\_\-gputensor@{format\_\-gputensor}}
\index{format\_\-gputensor@{format\_\-gputensor}!gputil.h@{gputil.h}}
\subsubsection[{format\_\-gputensor}]{\setlength{\rightskip}{0pt plus 5cm}void format\_\-gputensor ({\bf tensor} $\ast$ {\em t}, \/  FILE $\ast$ {\em out})}}
\label{gputil_8h_a0c42e9df93568af4129f42ef2efbde36}
\begin{DoxyInternal}{For internal use only.}
Debug function for printing gpu tensors without first having to copy them to host memory manually. \end{DoxyInternal}
\hypertarget{gputil_8h_acc9df64ede1f195cd8188a42a937972f}{
\index{gputil.h@{gputil.h}!gpu\_\-array\_\-get@{gpu\_\-array\_\-get}}
\index{gpu\_\-array\_\-get@{gpu\_\-array\_\-get}!gputil.h@{gputil.h}}
\subsubsection[{gpu\_\-array\_\-get}]{\setlength{\rightskip}{0pt plus 5cm}float gpu\_\-array\_\-get (float $\ast$ {\em dataptr}, \/  int {\em index})}}
\label{gputil_8h_acc9df64ede1f195cd8188a42a937972f}
\begin{DoxyInternal}{For internal use only.}
Reads one float from a GPU array, not extremely efficient. \end{DoxyInternal}
\hypertarget{gputil_8h_a6f2f5fc9528a4f1e2fc22f1ff8ff7a6e}{
\index{gputil.h@{gputil.h}!gpu\_\-array\_\-set@{gpu\_\-array\_\-set}}
\index{gpu\_\-array\_\-set@{gpu\_\-array\_\-set}!gputil.h@{gputil.h}}
\subsubsection[{gpu\_\-array\_\-set}]{\setlength{\rightskip}{0pt plus 5cm}void gpu\_\-array\_\-set (float $\ast$ {\em dataptr}, \/  int {\em index}, \/  float {\em value})}}
\label{gputil_8h_a6f2f5fc9528a4f1e2fc22f1ff8ff7a6e}
\begin{DoxyInternal}{For internal use only.}
Writes one float to a GPU array, not extremely efficient. \end{DoxyInternal}
\hypertarget{gputil_8h_aa843a5123b95fbe239b342f375e8a6c2}{
\index{gputil.h@{gputil.h}!gpu\_\-checkconf@{gpu\_\-checkconf}}
\index{gpu\_\-checkconf@{gpu\_\-checkconf}!gputil.h@{gputil.h}}
\subsubsection[{gpu\_\-checkconf}]{\setlength{\rightskip}{0pt plus 5cm}void gpu\_\-checkconf (dim3 {\em gridsize}, \/  dim3 {\em blocksize})}}
\label{gputil_8h_aa843a5123b95fbe239b342f375e8a6c2}
Checks if the CUDA 3D kernel launch configuration is valid. CUDA tends to ignore invalid configurations silently, which is painfull for debugging. \begin{Desc}
\item[\hyperlink{deprecated__deprecated000001}{Deprecated}]use \hyperlink{gputil_8h_adc4d57eeb6772ef2ac09f18e384c439c}{check3dconf()}, which uses the actual device properties \end{Desc}

\begin{DoxyParams}{Parameters}
\item[{\em gridsize}]3D size of the thread grid \item[{\em blocksize}]3D size of the trhead blocks on the grid \end{DoxyParams}
\hypertarget{gputil_8h_a7b70f435878eea3e2c5476581369e37f}{
\index{gputil.h@{gputil.h}!gpu\_\-checkconf\_\-int@{gpu\_\-checkconf\_\-int}}
\index{gpu\_\-checkconf\_\-int@{gpu\_\-checkconf\_\-int}!gputil.h@{gputil.h}}
\subsubsection[{gpu\_\-checkconf\_\-int}]{\setlength{\rightskip}{0pt plus 5cm}void gpu\_\-checkconf\_\-int (int {\em gridsize}, \/  int {\em blocksize})}}
\label{gputil_8h_a7b70f435878eea3e2c5476581369e37f}
Checks if the CUDA 1D kernel launch configuration is valid. CUDA tends to ignore invalid configurations silently, which is painfull for debugging. \begin{Desc}
\item[\hyperlink{deprecated__deprecated000002}{Deprecated}]use \hyperlink{gputil_8h_adfe62deb142d77cd6b63e3b6a0bf65ba}{check1dconf()}, which uses the actual device properties \end{Desc}

\begin{DoxyParams}{Parameters}
\item[{\em gridsize}]1D size of the thread grid \item[{\em blocksize}]1D size of the trhead blocks on the grid \end{DoxyParams}
\hypertarget{gputil_8h_a137892fd843da0d82f459f90bb2af209}{
\index{gputil.h@{gputil.h}!gpu\_\-getproperties@{gpu\_\-getproperties}}
\index{gpu\_\-getproperties@{gpu\_\-getproperties}!gputil.h@{gputil.h}}
\subsubsection[{gpu\_\-getproperties}]{\setlength{\rightskip}{0pt plus 5cm}void$\ast$ gpu\_\-getproperties (void)}}
\label{gputil_8h_a137892fd843da0d82f459f90bb2af209}
\begin{DoxyInternal}{For internal use only.}
Returns a cudaDeviceProp struct that contains the properties of the used GPU. When there are multiple GPUs present, the active one, used by this thread, is considered.

\begin{DoxyWarning}{Warning}
One global cudaDeviceProp$\ast$ is stored. The first time this function is called, it gets initialized. All subsequent calls return this cached cudaDeviceProp$\ast$. Consequently, the returned pointer must not be freed!
\end{DoxyWarning}
The struct looks like this: 
\begin{DoxyCode}
    char name[256];
    size_t totalGlobalMem;
    size_t sharedMemPerBlock;
    int regsPerBlock;
    int warpSize;
    size_t memPitch;
    int maxThreadsPerBlock;
    int maxThreadsDim[3];
    int maxGridSize[3];
    size_t totalConstMem;
    int major;
    int minor;
    int clockRate;
    size_t textureAlignment;
    int deviceOverlap;
    int multiProcessorCount;
    int kernelExecTimeoutEnabled;
    int integrated;
    int canMapHostMemory;
    int computeMode;
    int concurrentKernels;
\end{DoxyCode}


\begin{DoxyNote}{Note}
I currently return the cudaDeviceProp$\ast$ as a void$\ast$. In this way, none of the core functions expose cuda stuff directly. This makes it easier to link them with external code (Go, in my case). Arne. 
\end{DoxyNote}
\end{DoxyInternal}
\hypertarget{gputil_8h_a9b1f5575ea6503ba9152c1db1fb28cb3}{
\index{gputil.h@{gputil.h}!gpu\_\-override\_\-stride@{gpu\_\-override\_\-stride}}
\index{gpu\_\-override\_\-stride@{gpu\_\-override\_\-stride}!gputil.h@{gputil.h}}
\subsubsection[{gpu\_\-override\_\-stride}]{\setlength{\rightskip}{0pt plus 5cm}void gpu\_\-override\_\-stride (int {\em nFloats})}}
\label{gputil_8h_a9b1f5575ea6503ba9152c1db1fb28cb3}
\begin{DoxyInternal}{For internal use only.}
For debugging, it is handy to use a smaller-\/than-\/optimal stride; this prevents small test data to be padded to huge proportions. To reset to the intrinsic machine stride, set the value to -\/1. \end{DoxyInternal}

\begin{DoxyParams}{Parameters}
\item[{\em nFloats}]The stride (in number of floats) to use instead of the real, GPU-\/dependent stride. \end{DoxyParams}
\hypertarget{gputil_8h_a39fd2af01f3bfd02cf6001c270f652f8}{
\index{gputil.h@{gputil.h}!gpu\_\-pad\_\-to\_\-stride@{gpu\_\-pad\_\-to\_\-stride}}
\index{gpu\_\-pad\_\-to\_\-stride@{gpu\_\-pad\_\-to\_\-stride}!gputil.h@{gputil.h}}
\subsubsection[{gpu\_\-pad\_\-to\_\-stride}]{\setlength{\rightskip}{0pt plus 5cm}int gpu\_\-pad\_\-to\_\-stride (int {\em nFloats})}}
\label{gputil_8h_a39fd2af01f3bfd02cf6001c270f652f8}
This function takes an array size (in number of floats) and returns an array size -\/usually larger-\/ that can store the original array and fits the GPU stride. Example (for a stride of 64 floats -\/-\/ 256 bytes): 
\begin{DoxyCode}
  1 -> 64
  2 -> 64
 ...
 63 -> 64
 64 -> 64
 65 -> 128
 ...
\end{DoxyCode}
 \hypertarget{gputil_8h_aadb9a19a5d27210b0ed4dc9a04665826}{
\index{gputil.h@{gputil.h}!gpu\_\-safe@{gpu\_\-safe}}
\index{gpu\_\-safe@{gpu\_\-safe}!gputil.h@{gputil.h}}
\subsubsection[{gpu\_\-safe}]{\setlength{\rightskip}{0pt plus 5cm}void gpu\_\-safe (int {\em status})}}
\label{gputil_8h_aadb9a19a5d27210b0ed4dc9a04665826}
This function should be wrapped around cuda functions to check for a non-\/zero error status. It will print an error message and abort when neccesary. 
\begin{DoxyCode}
 gpu_safe( cudaMalloc(...) );
\end{DoxyCode}
 
\begin{DoxyParams}{Parameters}
\item[{\em status}]CUDA return status \end{DoxyParams}
\hypertarget{gputil_8h_ade18ea7bbea940fcdc15c47f6d05b3e7}{
\index{gputil.h@{gputil.h}!gpu\_\-stride\_\-float@{gpu\_\-stride\_\-float}}
\index{gpu\_\-stride\_\-float@{gpu\_\-stride\_\-float}!gputil.h@{gputil.h}}
\subsubsection[{gpu\_\-stride\_\-float}]{\setlength{\rightskip}{0pt plus 5cm}int gpu\_\-stride\_\-float ()}}
\label{gputil_8h_ade18ea7bbea940fcdc15c47f6d05b3e7}
Returns the optimal array stride (in number of floats): the second dimension of a 2D array should be a multiple of the stride. This number is usually 64 but could depend on the hardware.

E.g.: it is better to use a 3 x 64 array than a 64 x 3.

This seems to generalize to higher dimensions: at least the last dimension should be a multiple of the stride. E.g.: Standard problem 4 ran about 4x faster when using a (3x) 1 x 32 x 128 geometry instead of (3x) 128 x 32 x 1 !

\begin{Desc}
\item[\hyperlink{todo__todo000018}{Todo}]use cudaGetDeviceProperties for this? \end{Desc}
\begin{DoxySeeAlso}{See also}
\hyperlink{gputil_8h_a39fd2af01f3bfd02cf6001c270f652f8}{gpu\_\-pad\_\-to\_\-stride()} 
\end{DoxySeeAlso}
\hypertarget{gputil_8h_ad3d70add006fc3a817675184ebd29fad}{
\index{gputil.h@{gputil.h}!gpu\_\-zero@{gpu\_\-zero}}
\index{gpu\_\-zero@{gpu\_\-zero}!gputil.h@{gputil.h}}
\subsubsection[{gpu\_\-zero}]{\setlength{\rightskip}{0pt plus 5cm}void gpu\_\-zero (float $\ast$ {\em data}, \/  int {\em nElements})}}
\label{gputil_8h_ad3d70add006fc3a817675184ebd29fad}
Set a range of floats on the GPU to zero. 
\begin{DoxyParams}{Parameters}
\item[{\em data}]data pointer on the GPU \item[{\em nElements}]number of floats (not bytes) to be zeroed \end{DoxyParams}
\hypertarget{gputil_8h_a26e74f76194a27b6f685a840a63fb678}{
\index{gputil.h@{gputil.h}!gpu\_\-zero\_\-tensor@{gpu\_\-zero\_\-tensor}}
\index{gpu\_\-zero\_\-tensor@{gpu\_\-zero\_\-tensor}!gputil.h@{gputil.h}}
\subsubsection[{gpu\_\-zero\_\-tensor}]{\setlength{\rightskip}{0pt plus 5cm}void gpu\_\-zero\_\-tensor ({\bf tensor} $\ast$ {\em t})}}
\label{gputil_8h_a26e74f76194a27b6f685a840a63fb678}
Sets all the tensor's elements to zero. The tensor should be allocated on the GPU. \hypertarget{gputil_8h_a126d013db24c5bdf7552dac353f0e1a8}{
\index{gputil.h@{gputil.h}!make1dconf@{make1dconf}}
\index{make1dconf@{make1dconf}!gputil.h@{gputil.h}}
\subsubsection[{make1dconf}]{\setlength{\rightskip}{0pt plus 5cm}void make1dconf (int {\em N}, \/  int $\ast$ {\em gridSize}, \/  int $\ast$ {\em blockSize})}}
\label{gputil_8h_a126d013db24c5bdf7552dac353f0e1a8}
Makes a 1D thread configuration suited for a float array of size N The returned configuration will:
\begin{DoxyItemize}
\item span the entire array
\item have the largest valid block size that fits in the array
\item be valid
\end{DoxyItemize}

\begin{DoxySeeAlso}{See also}
\hyperlink{gputil_8h_a5f10ebe67635ce652fa229fa9d8ebc78}{make3dconf()}
\end{DoxySeeAlso}
Example: 
\begin{DoxyCode}
 int gridSize, blockSize;
 make1dconf(arraySize, &gridSize, &blockSize);
 mykernel<<<gridSize, blockSize>>>(arrrrgh);
\end{DoxyCode}
 
\begin{DoxyParams}{Parameters}
\item[{\em N}]size of array to span (number of floats) \item[{\em gridSize}]grid size is returned here \item[{\em blockSize}]block size is returned here \end{DoxyParams}
\hypertarget{gputil_8h_a5f10ebe67635ce652fa229fa9d8ebc78}{
\index{gputil.h@{gputil.h}!make3dconf@{make3dconf}}
\index{make3dconf@{make3dconf}!gputil.h@{gputil.h}}
\subsubsection[{make3dconf}]{\setlength{\rightskip}{0pt plus 5cm}void make3dconf (int {\em N0}, \/  int {\em N1}, \/  int {\em N2}, \/  dim3 $\ast$ {\em gridSize}, \/  dim3 $\ast$ {\em blockSize})}}
\label{gputil_8h_a5f10ebe67635ce652fa229fa9d8ebc78}
Makes a 3D thread configuration suited for a float array of size N0 x N1 x N2. The returned configuration will:
\begin{DoxyItemize}
\item span the entire N0 x N1 x N2 array
\item have the largest valid block size that fits in the N0 x N1 x N2 array
\item be valid
\end{DoxyItemize}

\begin{Desc}
\item[\hyperlink{todo__todo000019}{Todo}]works only up to N2 = 512 \end{Desc}
\begin{DoxySeeAlso}{See also}
\hyperlink{gputil_8h_a126d013db24c5bdf7552dac353f0e1a8}{make1dconf()}
\end{DoxySeeAlso}
Example: 
\begin{DoxyCode}
  dim3 gridSize, blockSize;
  make3dconf(N0, N1, N2, &gridSize, &blockSize);
  mykernel<<<gridSize, blockSize>>>(arrrrgh);
  
  __global__ void mykernel(aaarghs){
    
    int i = ((blockIdx.x * blockDim.x) + threadIdx.x)
    int j = ((blockIdx.y * blockDim.y) + threadIdx.y)
    int k = ((blockIdx.z * blockDim.z) + threadIdx.z)
    
    ...
  }
\end{DoxyCode}
 
\begin{DoxyParams}{Parameters}
\item[{\em N0}]size of 3D array to span \item[{\em N1}]size of 3D array to span \item[{\em N2}]size of 3D array to span \item[{\em gridSize}]grid size is returned here \item[{\em blockSize}]block size is returned here \end{DoxyParams}
\hypertarget{gputil_8h_a587c68bc938e76efa6982de793acf53d}{
\index{gputil.h@{gputil.h}!memcpy\_\-from\_\-gpu@{memcpy\_\-from\_\-gpu}}
\index{memcpy\_\-from\_\-gpu@{memcpy\_\-from\_\-gpu}!gputil.h@{gputil.h}}
\subsubsection[{memcpy\_\-from\_\-gpu}]{\setlength{\rightskip}{0pt plus 5cm}void memcpy\_\-from\_\-gpu (float $\ast$ {\em source}, \/  float $\ast$ {\em dest}, \/  int {\em nElements})}}
\label{gputil_8h_a587c68bc938e76efa6982de793acf53d}
Copies floats from GPU to the main RAM. \begin{DoxySeeAlso}{See also}
\hyperlink{gputil_8h_a9fc3a83fc4e2798b4a70fe5538d3db8f}{memcpy\_\-to\_\-gpu()}, \hyperlink{gputil_8h_abb3d67b2b6089e1620679ce812e982fa}{memcpy\_\-gpu\_\-to\_\-gpu()} 
\end{DoxySeeAlso}

\begin{DoxyParams}{Parameters}
\item[{\em source}]source data pointer on the GPU \item[{\em dest}]destination data pointer in the RAM \item[{\em nElements}]number of floats (not bytes) to be copied \end{DoxyParams}
\hypertarget{gputil_8h_abb3d67b2b6089e1620679ce812e982fa}{
\index{gputil.h@{gputil.h}!memcpy\_\-gpu\_\-to\_\-gpu@{memcpy\_\-gpu\_\-to\_\-gpu}}
\index{memcpy\_\-gpu\_\-to\_\-gpu@{memcpy\_\-gpu\_\-to\_\-gpu}!gputil.h@{gputil.h}}
\subsubsection[{memcpy\_\-gpu\_\-to\_\-gpu}]{\setlength{\rightskip}{0pt plus 5cm}void memcpy\_\-gpu\_\-to\_\-gpu (float $\ast$ {\em source}, \/  float $\ast$ {\em dest}, \/  int {\em nElements})}}
\label{gputil_8h_abb3d67b2b6089e1620679ce812e982fa}
Copies floats from GPU to GPU. \begin{DoxySeeAlso}{See also}
\hyperlink{gputil_8h_a9fc3a83fc4e2798b4a70fe5538d3db8f}{memcpy\_\-to\_\-gpu()}, \hyperlink{gputil_8h_a587c68bc938e76efa6982de793acf53d}{memcpy\_\-from\_\-gpu()} 
\end{DoxySeeAlso}

\begin{DoxyParams}{Parameters}
\item[{\em source}]source data pointer on the GPU \item[{\em dest}]destination data pointer on the GPU \item[{\em nElements}]number of floats (not bytes) to be copied \end{DoxyParams}
\hypertarget{gputil_8h_a9fc3a83fc4e2798b4a70fe5538d3db8f}{
\index{gputil.h@{gputil.h}!memcpy\_\-to\_\-gpu@{memcpy\_\-to\_\-gpu}}
\index{memcpy\_\-to\_\-gpu@{memcpy\_\-to\_\-gpu}!gputil.h@{gputil.h}}
\subsubsection[{memcpy\_\-to\_\-gpu}]{\setlength{\rightskip}{0pt plus 5cm}void memcpy\_\-to\_\-gpu (float $\ast$ {\em source}, \/  float $\ast$ {\em dest}, \/  int {\em nElements})}}
\label{gputil_8h_a9fc3a83fc4e2798b4a70fe5538d3db8f}
Copies floats from the main RAM to the GPU. \begin{DoxySeeAlso}{See also}
\hyperlink{gputil_8h_a587c68bc938e76efa6982de793acf53d}{memcpy\_\-from\_\-gpu()}, \hyperlink{gputil_8h_abb3d67b2b6089e1620679ce812e982fa}{memcpy\_\-gpu\_\-to\_\-gpu()} 
\end{DoxySeeAlso}

\begin{DoxyParams}{Parameters}
\item[{\em source}]source data pointer in the RAM \item[{\em dest}]destination data pointer on the GPU \item[{\em nElements}]number of floats (not bytes) to be copied \end{DoxyParams}
\hypertarget{gputil_8h_ac0a2a829eda78d8c61d65abe004c125e}{
\index{gputil.h@{gputil.h}!new\_\-gpu\_\-array@{new\_\-gpu\_\-array}}
\index{new\_\-gpu\_\-array@{new\_\-gpu\_\-array}!gputil.h@{gputil.h}}
\subsubsection[{new\_\-gpu\_\-array}]{\setlength{\rightskip}{0pt plus 5cm}float$\ast$ new\_\-gpu\_\-array (int {\em size})}}
\label{gputil_8h_ac0a2a829eda78d8c61d65abe004c125e}
Allocates an array of floats on the GPU and asserts the size is a multiple of 512. \begin{DoxySeeAlso}{See also}
\hyperlink{gputil_8h_a9bad0e9d5c5e49d54cc7f681954a990c}{new\_\-ram\_\-array()} 
\end{DoxySeeAlso}

\begin{DoxyParams}{Parameters}
\item[{\em size}]size of the array \end{DoxyParams}
\hypertarget{gputil_8h_a7c2b7f29fd636e841b2001b07ffb5d18}{
\index{gputil.h@{gputil.h}!new\_\-gputensor@{new\_\-gputensor}}
\index{new\_\-gputensor@{new\_\-gputensor}!gputil.h@{gputil.h}}
\subsubsection[{new\_\-gputensor}]{\setlength{\rightskip}{0pt plus 5cm}{\bf tensor}$\ast$ new\_\-gputensor (int {\em rank}, \/  int $\ast$ {\em size})}}
\label{gputil_8h_a7c2b7f29fd636e841b2001b07ffb5d18}
Creates a new tensor whose data is allocated on the GPU. (rank and size are stored in the host RAM) \begin{Desc}
\item[\hyperlink{todo__todo000017}{Todo}]delete\_\-gputensor() \end{Desc}
\hypertarget{gputil_8h_a9bad0e9d5c5e49d54cc7f681954a990c}{
\index{gputil.h@{gputil.h}!new\_\-ram\_\-array@{new\_\-ram\_\-array}}
\index{new\_\-ram\_\-array@{new\_\-ram\_\-array}!gputil.h@{gputil.h}}
\subsubsection[{new\_\-ram\_\-array}]{\setlength{\rightskip}{0pt plus 5cm}float$\ast$ new\_\-ram\_\-array (int {\em size})}}
\label{gputil_8h_a9bad0e9d5c5e49d54cc7f681954a990c}
Allocates an array of floats in the main RAM. \begin{DoxySeeAlso}{See also}
\hyperlink{gputil_8h_ac0a2a829eda78d8c61d65abe004c125e}{new\_\-gpu\_\-array()} 
\end{DoxySeeAlso}

\begin{DoxyParams}{Parameters}
\item[{\em size}]size of the array \end{DoxyParams}
\hypertarget{gputil_8h_a07dc809bc071234d5b1b2af3dd292860}{
\index{gputil.h@{gputil.h}!print\_\-device\_\-properties@{print\_\-device\_\-properties}}
\index{print\_\-device\_\-properties@{print\_\-device\_\-properties}!gputil.h@{gputil.h}}
\subsubsection[{print\_\-device\_\-properties}]{\setlength{\rightskip}{0pt plus 5cm}void print\_\-device\_\-properties (FILE $\ast$ {\em out})}}
\label{gputil_8h_a07dc809bc071234d5b1b2af3dd292860}
Prints the properties of the used GPU 
\begin{DoxyParams}{Parameters}
\item[{\em out}]stream to print to \end{DoxyParams}
\hypertarget{gputil_8h_ad00a2a5bb51df697f7abd399431962fd}{
\index{gputil.h@{gputil.h}!print\_\-device\_\-properties\_\-stdout@{print\_\-device\_\-properties\_\-stdout}}
\index{print\_\-device\_\-properties\_\-stdout@{print\_\-device\_\-properties\_\-stdout}!gputil.h@{gputil.h}}
\subsubsection[{print\_\-device\_\-properties\_\-stdout}]{\setlength{\rightskip}{0pt plus 5cm}void print\_\-device\_\-properties\_\-stdout ()}}
\label{gputil_8h_ad00a2a5bb51df697f7abd399431962fd}
\hypertarget{gputil_8h_ad212066ef724f8314ca9f50148c7ebba}{
\index{gputil.h@{gputil.h}!tensor\_\-copy\_\-from\_\-gpu@{tensor\_\-copy\_\-from\_\-gpu}}
\index{tensor\_\-copy\_\-from\_\-gpu@{tensor\_\-copy\_\-from\_\-gpu}!gputil.h@{gputil.h}}
\subsubsection[{tensor\_\-copy\_\-from\_\-gpu}]{\setlength{\rightskip}{0pt plus 5cm}void tensor\_\-copy\_\-from\_\-gpu ({\bf tensor} $\ast$ {\em source}, \/  {\bf tensor} $\ast$ {\em dest})}}
\label{gputil_8h_ad212066ef724f8314ca9f50148c7ebba}
Copies the source tensor (on the GPU) to the the destination tensor (in RAM). They should have equal sizes. \begin{DoxySeeAlso}{See also}
\hyperlink{gputil_8h_a8ca9dcaab221f4cf101e18c9dd64612b}{tensor\_\-copy\_\-to\_\-gpu()}, \hyperlink{gputil_8h_ab771627ffcf287025e3b9d0a0558c541}{tensor\_\-copy\_\-gpu\_\-to\_\-gpu()} 
\end{DoxySeeAlso}
\hypertarget{gputil_8h_ab771627ffcf287025e3b9d0a0558c541}{
\index{gputil.h@{gputil.h}!tensor\_\-copy\_\-gpu\_\-to\_\-gpu@{tensor\_\-copy\_\-gpu\_\-to\_\-gpu}}
\index{tensor\_\-copy\_\-gpu\_\-to\_\-gpu@{tensor\_\-copy\_\-gpu\_\-to\_\-gpu}!gputil.h@{gputil.h}}
\subsubsection[{tensor\_\-copy\_\-gpu\_\-to\_\-gpu}]{\setlength{\rightskip}{0pt plus 5cm}void tensor\_\-copy\_\-gpu\_\-to\_\-gpu ({\bf tensor} $\ast$ {\em source}, \/  {\bf tensor} $\ast$ {\em dest})}}
\label{gputil_8h_ab771627ffcf287025e3b9d0a0558c541}
Copies the source tensor to the the destination tensor (both on the GPU). They should have equal sizes. \begin{DoxySeeAlso}{See also}
\hyperlink{gputil_8h_a8ca9dcaab221f4cf101e18c9dd64612b}{tensor\_\-copy\_\-to\_\-gpu()}, \hyperlink{gputil_8h_ad212066ef724f8314ca9f50148c7ebba}{tensor\_\-copy\_\-from\_\-gpu()} 
\end{DoxySeeAlso}
\hypertarget{gputil_8h_a8ca9dcaab221f4cf101e18c9dd64612b}{
\index{gputil.h@{gputil.h}!tensor\_\-copy\_\-to\_\-gpu@{tensor\_\-copy\_\-to\_\-gpu}}
\index{tensor\_\-copy\_\-to\_\-gpu@{tensor\_\-copy\_\-to\_\-gpu}!gputil.h@{gputil.h}}
\subsubsection[{tensor\_\-copy\_\-to\_\-gpu}]{\setlength{\rightskip}{0pt plus 5cm}void tensor\_\-copy\_\-to\_\-gpu ({\bf tensor} $\ast$ {\em source}, \/  {\bf tensor} $\ast$ {\em dest})}}
\label{gputil_8h_a8ca9dcaab221f4cf101e18c9dd64612b}
Copies the source tensor (in RAM) to the the destination tensor (on the GPU). They should have equal sizes. \begin{DoxySeeAlso}{See also}
\hyperlink{gputil_8h_ad212066ef724f8314ca9f50148c7ebba}{tensor\_\-copy\_\-from\_\-gpu()}, \hyperlink{gputil_8h_ab771627ffcf287025e3b9d0a0558c541}{tensor\_\-copy\_\-gpu\_\-to\_\-gpu()} 
\end{DoxySeeAlso}
