.TH "gpuconv2.h" 3 "6 Jul 2010" "GPU_simulations" \" -*- nroff -*-
.ad l
.nh
.SH NAME
gpuconv2.h \- 
.SH SYNOPSIS
.br
.PP
\fC#include 'tensor.h'\fP
.br
\fC#include 'gputil.h'\fP
.br
\fC#include <cufft.h>\fP
.br
\fC#include 'gpufft2.h'\fP
.br

.SS "Data Structures"

.in +1c
.ti -1c
.RI "struct \fBgpuconv2\fP"
.br
.in -1c
.SS "Functions"

.in +1c
.ti -1c
.RI "\fBgpuconv2\fP * \fBnew_gpuconv2\fP (int *size, int *kernelSize)"
.br
.ti -1c
.RI "void \fBgpuconv2_loadkernel5DSymm\fP (\fBgpuconv2\fP *conv, \fBtensor\fP *kernel5D)"
.br
.ti -1c
.RI "void \fBgpuconv2_loadkernel2DSymm\fP (\fBgpuconv2\fP *conv, \fBtensor\fP *kernel2D)"
.br
.ti -1c
.RI "void \fBgpuconv2_exec\fP (\fBgpuconv2\fP *plan, \fBtensor\fP *source, \fBtensor\fP *dest)"
.br
.ti -1c
.RI "void \fBgpuconv2_loadkernel\fP (\fBgpuconv2\fP *plan, \fBtensor\fP *kernel)"
.br
.ti -1c
.RI "void \fBgpu_kernel_mul2\fP (float *ft_m_i, float *ft_kernel_comp_ij, float *ft_h_comp_j, int nRealNumbers)"
.br
.ti -1c
.RI "void \fBgpu_kernel_mul_complex_inplace_symm\fP (float *fftMx, float *fftMy, float *fftMz, float *fftKxx, float *fftKyy, float *fftKzz, float *fftKyz, float *fftKxz, float *fftKxy, int nRealNumbers)"
.br
.ti -1c
.RI "void \fBgpuconv2_copy_pad\fP (\fBgpuconv2\fP *conv, float *source, float *dest)"
.br
.ti -1c
.RI "void \fBgpuconv2_copy_unpad\fP (\fBgpuconv2\fP *conv, float *source, float *dest)"
.br
.in -1c
.SH "Detailed Description"
.PP 
A smarter vector convolution plan on the GPU: real-to-complex FFT's. The redundant zero's in the padded magnetization buffers are ignored. The zero's in the micromagnetic kernel are ignored. Care is taken to align CUDA memory access.
.PP
The interface is flexible: gpuconv2_exec(m, h) can be called on any magnetization and field array that match the size of the plan. m and h are thus not stored in the plan itself. This is handy for higher order solvers that keep multiple versions of m and h.
.PP
\fBTodo\fP
.RS 4
TODO voor convolutie (Ben)
.RE
.PP
1. Greense functies -> dienen gegenereerd te worden in strided formaat -> is symmetrische rank 4 tensor (vb: gxy = gyx, gxz = gzx, ..., slechts 2/3 van geheugen nodig) -> Enkel reeel deel in Fourier domein (halveert geheugen vereisten) -> implementatie algemene Greense tensor nodig met als input te gebruiken Greense functie -> Er dient rekening gehouden te worden met mogelijke periodiciteit
.PP
2. seriele berekening veldwaarden gunstiger -> beter seriele berekening van H_x, H_y, H_z als a. H^FFT = g^FFT_xx* m^FFT_x + g^FFT_xy* m^FFT_y + g^FFT_xz* m^FFT_z b. H_x = inv_FFT(H^FFT) c. H^FFT = g^FFT_xy* m^FFT_x + g^FFT_yy* m^FFT_y + g^FFT_yz* m^FFT_z d. H_y = inv_FFT(H^FFT) e. H^FFT = g^FFT_xz* m^FFT_x + g^FFT_yz* m^FFT_y + g^FFT_zz* m^FFT_z f. H_z = inv_FFT(H^FFT) Op die manier enkel geheugen nodig voor H^FFT (en niet voor elke component H^FFT_x, H^FFT_y, H^FFT_z) Antw: Ik denk dat ik nu slechts even veel geheugen gebruik: Ik houd 3 H^FFT componenten in het geheugen, maar slechts één m^FFT component, jij één H^FFT maar 3 m^FFT's. Of heb ik het mis op? (Arne.) Opm: misschien kunnen we wel één buffer uitsparen door eerst alle m_i te FFT-en en dan een 'in-place' kernel vermenigvuldiging te doen. Per element wordt dan m_x[i], m_y[i], m_z[i] gebufferd in locale variablen, daarna wordt m^FFT element per element overschreven door H^FFT...
.PP
-> H^FFT dient dezelfde dimensies te hebben als andere strided FFT tensoren
.PP
3. Transponeren matrices -> is versnelling mogelijk door nullen niet te transponeren? -> In place transponeren
.PP
4. Omtrent de FFT routines -> Waarschijnlijk beter om FFT routines in een aparte bibliotheek te steken wegens mogelijk gebruik in andere convoluties -> implementatie 2D varianten: Uitbreiding van de huidige routines of aparte routines? mogelijkheden: a. Aparte routines voor 3D en 2D: bij aanroepen if constructies nodig (if 3D, if 2D) b. uitbreiding routines:
.IP "\(bu" 2
extra argument 2D of 3D, met daarna daarna twee totaal verschillende code blokken
.IP "\(bu" 2
geen extra argumenten, maar op basis van dimensies in argument.
.PP
.PP
\fBSee also:\fP
.RS 4
\fBgpuconv1\fP, \fBnew_gpuconv2\fP, \fBgpuconv2_exec\fP
.RE
.PP
\fBTodo\fP
.RS 4
when NO=1, gxy and gxz are zero
.RE
.PP
\fBAuthor:\fP
.RS 4
Arne Vansteenkiste 
.PP
Ben Van de Wiele 
.RE
.PP

.SH "Function Documentation"
.PP 
.SS "void gpu_kernel_mul2 (float * ft_m_i, float * ft_kernel_comp_ij, float * ft_h_comp_j, int nRealNumbers)"Pointwise multiplication of arrays of complex numbers. ft_h_comp_j += ft_m_i * ft_kernel_comp_ij. Runs on the GPU. Makes use of kernel symmetry 
.PP
\fBNote:\fP
.RS 4
DO NOT store in texture memory! This would be a bit faster on older devices, but actually slower on Fermi cards! 
.RE
.PP
\fBParameters:\fP
.RS 4
\fIft_m_i\fP multiplication input 1 
.br
\fIft_kernel_comp_ij\fP multiplication input 2 
.br
\fIft_h_comp_j\fP multiplication result gets added to this array 
.br
\fInRealNumbers\fP the number of floats(!) in each of the arrays, thus twice the number of complex's in them. 
.RE
.PP

.SS "void gpu_kernel_mul_complex_inplace_symm (float * fftMx, float * fftMy, float * fftMz, float * fftKxx, float * fftKyy, float * fftKzz, float * fftKyz, float * fftKxz, float * fftKxy, int nRealNumbers)".PP
\fBFor internal use only.\fP
.RS 4
.RE
.PP

.SS "void gpuconv2_copy_pad (\fBgpuconv2\fP * conv, float * source, float * dest)".PP
\fBFor internal use only.\fP
.RS 4
Copies 3D data to a zero-padded, strided destination. Runs on the GPU. 
.RE
.PP
\fBParameters:\fP
.RS 4
\fIconv\fP this convolution plan contains the sizes of both arrays 
.br
\fIsource\fP source data on GPU, should have size: conv->size 
.br
\fIdest\fP destination data on GPU, should have size: conv->paddedStorageSize 
.RE
.PP

.SS "void gpuconv2_copy_unpad (\fBgpuconv2\fP * conv, float * source, float * dest)".PP
\fBFor internal use only.\fP
.RS 4
Copies 3D data from a zero-padded and strided destination. Runs on the GPU 
.RE
.PP
\fBParameters:\fP
.RS 4
\fIconv\fP this convolution plan contains the sizes of both arrays 
.br
\fIsource\fP destination data on GPU, should have size: conv->paddedStorageSize 
.br
\fIdest\fP source data on GPU, should have size: conv->size 
.RE
.PP

.SS "void gpuconv2_exec (\fBgpuconv2\fP * plan, \fBtensor\fP * source, \fBtensor\fP * dest)"Executes the convolution plan: convolves the source data with the stored kernel and stores the result in the destination pointer. \fBParameters:\fP
.RS 4
\fIplan\fP the plan to execute 
.br
\fIsource\fP the input vector field (magnetization) 
.br
\fIdest\fP the destination vector field (magnetic field) to store the result in 
.RE
.PP

.SS "void gpuconv2_loadkernel (\fBgpuconv2\fP * plan, \fBtensor\fP * kernel)"Loads a kernel. Automatically called during \fBnew_gpuconv2()\fP, but could be used to change the kernel afterwards. 
.PP
\fBSee also:\fP
.RS 4
\fBnew_gpuconv2\fP 
.RE
.PP
\fBParameters:\fP
.RS 4
\fIplan\fP plan to load the kernel into 
.br
\fIkernel\fP kernel to load (should match the plan size) 
.RE
.PP

.SS "void gpuconv2_loadkernel2DSymm (\fBgpuconv2\fP * conv, \fBtensor\fP * kernel2D)"Loads a kernel into the convolution. The kernel is FFTed and stored in a 2-dimensional format: Kernel[SourceDir][index]. The kernel has the format discribed in \fBgpukernel1.h\fP 
.PP
\fBSee also:\fP
.RS 4
\fBgpukernel1.h\fP 
.RE
.PP
\fBNote:\fP
.RS 4
for use with Ben's kernels 
.RE
.PP
\fBTodo\fP
.RS 4
not yet implemented 
.RE
.PP
\fBParameters:\fP
.RS 4
\fIkernel2D\fP Kernel on Device, normalized 
.RE
.PP

.SS "void gpuconv2_loadkernel5DSymm (\fBgpuconv2\fP * conv, \fBtensor\fP * kernel5D)"Loads a kernel into the convolution. The kernel is not yet FFTed and stored in the 5-dimensional format: Kernel[SourceDir][DestDir][X][Y][Z]. The kernel is assumed to be symmetric in the first two indices. 
.PP
\fBNote:\fP
.RS 4
for use with Arne's kernels. 
.RE
.PP
\fBParameters:\fP
.RS 4
\fIkernel5D\fP Kernel on Host, not yet normalized 
.RE
.PP

.SS "\fBgpuconv2\fP* new_gpuconv2 (int * size, int * kernelSize)"New convolution plan with given size of the source vector field and kernel. If the kernel size is larger than the vector field, the field is zero-padded in the respective dimension to fit the size of the kernel. 
.PP
\fBNote:\fP
.RS 4
After construction, a kernel should still be loaded. 
.RE
.PP
\fBParameters:\fP
.RS 4
\fIsize\fP X Y and Z size of the magnetization vector field 
.br
\fIkernelSize\fP convolution kernel size of at least the size of the vector field 
.RE
.PP

.SH "Author"
.PP 
Generated automatically by Doxygen for GPU_simulations from the source code.
